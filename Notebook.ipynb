{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593313f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v1\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "# This is needed to get the input size for the NN\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "# This is needed to clip the actions within the legal boundaries\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a25ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a noise used in the paper that introduced DDPG https://arxiv.org/pdf/1509.02971v6.pdf\n",
    "# From what I understand, we can also use Gaussian or other noise without much difference,\n",
    "# I left the noise as is\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "493ba423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its important to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Makes a record of the outputted (s,a,r,s') obervation tuple\n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(layer1=400, layer2=300):\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Multiply to fill the whole action space which should be equal around 0\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic(layer1=400, layer2=300):\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Make it into a keras model\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765aff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002,\n",
    "            actor_lr=0.001, gamma=0.99, tau=0.005):\n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.std_dev = std_dev\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.actor_model = get_actor(layer1=400, layer2=300)\n",
    "        self.critic_model = get_critic(layer1=400, layer2=300)\n",
    "\n",
    "        self.target_actor = get_actor(layer1=400, layer2=300)\n",
    "        self.target_critic = get_critic(layer1=400, layer2=300)\n",
    "        \n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "        \n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "        \n",
    "        self.buffer = Buffer(buffer_capacity, batch_size)\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "    \n",
    "    # Move the update and learn function from buffer to Agent to \"decrease\" scope\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.buffer.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.buffer.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.buffer.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.buffer.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "        \n",
    "    def policy(self, state, noise_object, use_noise=True):\n",
    "        # For doing actions without added noise\n",
    "        if not use_noise:     \n",
    "            sampled_actions = tf.squeeze(actor_model(state)).numpy()\n",
    "            legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "            return [np.squeeze(legal_action)]\n",
    "        else:\n",
    "            sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "            noise = noise_object()\n",
    "            # Adding noise to action\n",
    "            sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "            # We make sure action is within bounds\n",
    "            legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "            return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae2ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This updates the weights in a slow manner which keeps stability\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee0ff23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(total_episodes=100):\n",
    "    # To store reward history of each episode\n",
    "    ep_reward_list = []\n",
    "    # To store average reward history of last few episodes\n",
    "    avg_reward_list = []\n",
    "    \n",
    "    #for trial in range(trial_times):\n",
    "\n",
    "    # add sublists for each trial\n",
    "    #avg_reward_list.append([])\n",
    "    #ep_reward_list.append([])\n",
    "    \n",
    "    agent = Agent()\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "\n",
    "        prev_state = env.reset()\n",
    "        episodic_reward = 0\n",
    "\n",
    "        while True:\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "            action = agent.policy(tf_prev_state, agent.ou_noise)\n",
    "            # Recieve state and reward from environment.\n",
    "            state, reward, done, info = env.step(action)\n",
    "\n",
    "            agent.buffer.record((prev_state, action, reward, state))\n",
    "            episodic_reward += reward\n",
    "\n",
    "            agent.learn()\n",
    "            update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "            update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "            # End this episode if en episode is done\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "\n",
    "        ep_reward_list.append(episodic_reward)\n",
    "\n",
    "        # Mean of last 40 episodes\n",
    "        avg_reward = np.mean(ep_reward_list[-40:])\n",
    "        print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        avg_reward_list.append(avg_reward)\n",
    "\n",
    "    # Plotting graph\n",
    "    # Episodes versus Avg. Rewards\n",
    "    #for idx, p in enumerate(avg_reward_list):\n",
    "    #    plt.plot(p, label=str(idx))\n",
    "    plt.plot(avg_reward_list)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Avg. Epsiodic Reward (40)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b89a8efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1238.3867322031756\n",
      "Episode * 1 * Avg Reward is ==> -1412.448269900367\n",
      "Episode * 2 * Avg Reward is ==> -1475.7152132234485\n",
      "Episode * 3 * Avg Reward is ==> -1560.7093235613993\n",
      "Episode * 4 * Avg Reward is ==> -1460.553985020637\n",
      "Episode * 5 * Avg Reward is ==> -1454.8727415524606\n",
      "Episode * 6 * Avg Reward is ==> -1434.4959798169546\n",
      "Episode * 7 * Avg Reward is ==> -1397.0311911065128\n",
      "Episode * 8 * Avg Reward is ==> -1363.6255811589695\n",
      "Episode * 9 * Avg Reward is ==> -1325.445961383128\n",
      "Episode * 10 * Avg Reward is ==> -1291.511243838687\n",
      "Episode * 11 * Avg Reward is ==> -1238.7232434055072\n",
      "Episode * 12 * Avg Reward is ==> -1185.3263842497354\n",
      "Episode * 13 * Avg Reward is ==> -1128.715903789653\n",
      "Episode * 14 * Avg Reward is ==> -1062.2256770822119\n",
      "Episode * 15 * Avg Reward is ==> -1012.1416238322818\n",
      "Episode * 16 * Avg Reward is ==> -960.6363175922783\n",
      "Episode * 17 * Avg Reward is ==> -920.8416525100458\n",
      "Episode * 18 * Avg Reward is ==> -914.132555221258\n",
      "Episode * 19 * Avg Reward is ==> -880.5135423157357\n",
      "Episode * 20 * Avg Reward is ==> -838.6510051731285\n",
      "Episode * 21 * Avg Reward is ==> -806.0872446307286\n",
      "Episode * 22 * Avg Reward is ==> -787.7289149833956\n",
      "Episode * 23 * Avg Reward is ==> -765.2030178554027\n",
      "Episode * 24 * Avg Reward is ==> -739.6413863418973\n",
      "Episode * 25 * Avg Reward is ==> -724.1418426732839\n",
      "Episode * 26 * Avg Reward is ==> -701.9809002987766\n",
      "Episode * 27 * Avg Reward is ==> -681.4160432874615\n",
      "Episode * 28 * Avg Reward is ==> -662.2108777588207\n",
      "Episode * 29 * Avg Reward is ==> -653.4829411099207\n",
      "Episode * 30 * Avg Reward is ==> -632.4187101632269\n",
      "Episode * 31 * Avg Reward is ==> -616.456109653607\n",
      "Episode * 32 * Avg Reward is ==> -601.4874412596993\n",
      "Episode * 33 * Avg Reward is ==> -591.054217636414\n",
      "Episode * 34 * Avg Reward is ==> -581.922977362204\n",
      "Episode * 35 * Avg Reward is ==> -569.0555972288614\n",
      "Episode * 36 * Avg Reward is ==> -553.697747142768\n",
      "Episode * 37 * Avg Reward is ==> -542.4619239409263\n",
      "Episode * 38 * Avg Reward is ==> -531.7114729724399\n",
      "Episode * 39 * Avg Reward is ==> -518.4284540456563\n",
      "Episode * 40 * Avg Reward is ==> -493.8974696366722\n",
      "Episode * 41 * Avg Reward is ==> -454.30339056333395\n",
      "Episode * 42 * Avg Reward is ==> -417.28130373315423\n",
      "Episode * 43 * Avg Reward is ==> -371.94977771860977\n",
      "Episode * 44 * Avg Reward is ==> -351.8804615329881\n",
      "Episode * 45 * Avg Reward is ==> -328.99980648331257\n",
      "Episode * 46 * Avg Reward is ==> -305.75694355496626\n",
      "Episode * 47 * Avg Reward is ==> -286.37347484916864\n",
      "Episode * 48 * Avg Reward is ==> -262.0756165485347\n",
      "Episode * 49 * Avg Reward is ==> -240.55393970980668\n",
      "Episode * 50 * Avg Reward is ==> -219.85980327513343\n",
      "Episode * 51 * Avg Reward is ==> -209.5321704764778\n",
      "Episode * 52 * Avg Reward is ==> -198.98262853316513\n",
      "Episode * 53 * Avg Reward is ==> -192.17776609066328\n",
      "Episode * 54 * Avg Reward is ==> -191.9788712305126\n",
      "Episode * 55 * Avg Reward is ==> -188.5801076677297\n",
      "Episode * 56 * Avg Reward is ==> -190.91111253006483\n",
      "Episode * 57 * Avg Reward is ==> -187.93427428375975\n",
      "Episode * 58 * Avg Reward is ==> -174.40439431417693\n",
      "Episode * 59 * Avg Reward is ==> -171.49372801629224\n",
      "Episode * 60 * Avg Reward is ==> -174.69228749302405\n",
      "Episode * 61 * Avg Reward is ==> -174.68488274369912\n",
      "Episode * 62 * Avg Reward is ==> -168.1893279288003\n",
      "Episode * 63 * Avg Reward is ==> -170.6118361338697\n",
      "Episode * 64 * Avg Reward is ==> -170.38003627786983\n",
      "Episode * 65 * Avg Reward is ==> -165.1177313563212\n",
      "Episode * 66 * Avg Reward is ==> -165.03226387674064\n",
      "Episode * 67 * Avg Reward is ==> -165.12428992036166\n",
      "Episode * 68 * Avg Reward is ==> -169.90363191701044\n",
      "Episode * 69 * Avg Reward is ==> -163.15380109639648\n",
      "Episode * 70 * Avg Reward is ==> -172.72416993010467\n",
      "Episode * 71 * Avg Reward is ==> -175.85615211460598\n",
      "Episode * 72 * Avg Reward is ==> -172.87048245011462\n",
      "Episode * 73 * Avg Reward is ==> -169.8561271992973\n",
      "Episode * 74 * Avg Reward is ==> -171.155810731989\n",
      "Episode * 75 * Avg Reward is ==> -168.3024646909218\n",
      "Episode * 76 * Avg Reward is ==> -171.35832794728717\n",
      "Episode * 77 * Avg Reward is ==> -177.2528018275228\n",
      "Episode * 78 * Avg Reward is ==> -177.39092660160372\n",
      "Episode * 79 * Avg Reward is ==> -180.45017526078624\n",
      "Episode * 80 * Avg Reward is ==> -177.249386333343\n",
      "Episode * 81 * Avg Reward is ==> -180.3156834063762\n",
      "Episode * 82 * Avg Reward is ==> -177.46032840935635\n",
      "Episode * 83 * Avg Reward is ==> -180.59625127442004\n",
      "Episode * 84 * Avg Reward is ==> -180.034251407018\n",
      "Episode * 85 * Avg Reward is ==> -170.27589584400698\n",
      "Episode * 86 * Avg Reward is ==> -166.98635008667898\n",
      "Episode * 87 * Avg Reward is ==> -164.316530215652\n",
      "Episode * 88 * Avg Reward is ==> -161.27848414033022\n",
      "Episode * 89 * Avg Reward is ==> -161.4611353108906\n",
      "Episode * 90 * Avg Reward is ==> -158.4483306214055\n",
      "Episode * 91 * Avg Reward is ==> -155.55760944374563\n",
      "Episode * 92 * Avg Reward is ==> -155.67049642440065\n",
      "Episode * 93 * Avg Reward is ==> -152.7461898830448\n",
      "Episode * 94 * Avg Reward is ==> -160.00496627951904\n",
      "Episode * 95 * Avg Reward is ==> -160.21125159232483\n",
      "Episode * 96 * Avg Reward is ==> -160.48659661510098\n",
      "Episode * 97 * Avg Reward is ==> -160.54388427958807\n",
      "Episode * 98 * Avg Reward is ==> -157.34938285720173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 99 * Avg Reward is ==> -157.44411270734386\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEGCAYAAACgt3iRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxBUlEQVR4nO3deXhU5dn48e+dhOyEbOxJCAIKKCAYEZRatWhxpa51acWVturPtm/7utS+rV1fta+2WltbqlioC7iDgiJYK1ZFCIssIiWsSQiYDQIJWef+/XFOdMQsQzJbZu7Pdc2VOc85Z849jsw9z3KeR1QVY4wxpjtiQh2AMcaYns+SiTHGmG6zZGKMMabbLJkYY4zpNksmxhhjui0u1AGESnZ2tubn54c6DGOM6VFWr15doap9jyyP2mSSn59PYWFhqMMwxpgeRUR2tVVuzVzGGGO6zZKJMcaYbrNkYowxptuits/EGGOiXVNTEyUlJdTX139pX2JiIjk5OfTq1cun17JkYowxUaqkpITevXuTn5+PiHxWrqpUVlZSUlLC0KFDfXota+YyxpgoVV9fT1ZW1hcSCYCIkJWV1WaNpT2WTIwxJoodmUg6K2+PNXMZY0wYUFX21zVxuKkFERCEytoGiqvq2F1VR1ZKAlNH96dPkm99GMFmycQYEzKt6ykd7a/gnqS+qYXdVXWUVh+m2aPECKhCWU09uytr2VXpJIviqjpqG1s6fK1escKU4dmMz8sgNkaIESFGIDZGPtt2EhHUNrawq7KW7eW17KupR3HKRYQ5108kLyvZr+/TkokxplMH65so3FlNYq9YBvZJZECfRHrFxtDiUZo9HjaXHWTljipW76qisrYR7zX3WvNE39QExudlMD4vHY9HefPjfSz9eB+HGpq5cmIu107OZ3B6UodxtHiU6rpGknrFkhwfi4hQU99EcVUdew/Uk5WaQH5WMunJ8QH8r/Fl1bWNbK84xPbyWnZW1lJSfZjS6sOUVB9mb037/Q7xcTHkZSYzJDOZScdkkZuZTEp8LAp4VMlIjicvM5ncjGS2Vxxi8YYyFm/Yy9tbyn2KKyslnvzsFMbkpH+WxBRI6PV5D4eqtpnMj3bhRInWlRYLCgrUplMx0ap0/2FeXF3ifPFVHabiUAOD0pMYmp1CXmYyvWKdL5e6phbeK6pg5Y4qmlo6/644pm/KZwlBRD77QlJ1rrmjovazYxPiYpgyPJv4uBiWbNqLiHD6iGyOHdCboVkpZKTEU1p9mN1VdeyqrGVXVR0lVYdpbPEAzq/xhLgY6tr4Nd8nqRcT8tKZMqIvU4ZnMyAtEYmBGBGSe8USE/PFL88m9zV7xX6xG1lV8ahzrSN5PMo7/ynn8X9v572iys/K42KEgemJDE5PYnB6MkOynEdORjLxsTF4VFGgf1oC/XsnfimWzqgqLR6lRRVVPnve0qKfvTY4iSotseMmsR07dtC7d+8vdcK3juY6ePDgl0ZzichqVS048rUsmRgTRZpaPDz53g5+v3Qrh5taGNgnkdzMZLJS4tmz/zDbK2o5WN/8hXNG9EvlrFH9+OoIZ26/sgP17K2pp7lFiY1xksYx2SkU5GfSt3dCh9evrm1kbXE1LR44bXgWyfFO40hJdR3/+GAXyzbvo9grYQCkxMeSl5XCkMxkhmQnMzAtkYZmDwfrm6lrbKF/WgK5mckM6JNI5aFGdlXWsq38ECu2V30hebVK7BXDkMwU8rKSaWz2fFaTABicnsSQLOdLf3dVHcXVdTS3KLmZTkLo1zuB5halocXDJ2U1bCuvpX9aAldPHMLYnD4MzU4hJyOJuNieMbapK/eZ9JhkIiK/Ay4EGoFtwPWqut/ddzdwI9AC3K6qS9zyacDDQCzwuKre19l1LJmYSFNV28j28kNsKz/EwfpmBvRJZEBaIvFxMe6v+zpe/WgPn+w9yNRR/bj3ouPJyfhiu7mqUlPfjMfjfC/ExEjQO3xbPMqe/YeprmtkcHoSmSnxXe5TKamu44NtldTUN7u1DOXTmgZ2Vtaxu6qWuJgYhvZNYWhWCiKwq7KOnZW1NLUouRlJ5GUmEx8X81l5xaEGesXGEB8XQ3ZKAlefksd5YwYSH9czkoc/9KRkcg7wT1VtFpH7AVT1ThEZDTwLTAQGAcuAY93T/gOcDZQAq4CrVPXjjq5jycT0RC0eZc3uat7ctJelH++jdP9hPOq0r/vyTzk/K5mfnDeKc44fEPhgTURqL5mEXQe8qr7ptbkCuMx9Ph2Yp6oNwA4RKcJJLABFqrodQETmucd2mEyM6Unqm1qYv6qYWcu3U7r/MPGxMUwelsW5YwYSFyOICGmJcQzrl8qw7FR6J8ax72A9ew/U09DscTpxM5NJTQi7f/ImQoT7/1k3APPd54NxkkurErcMoPiI8lPaejERmQnMBMjLy/NroMYEwsH6Jp75cDd/e3cHFYcaKBiSwV3njuSM4/rSu5PO1YyUeEYOSAtSpCbahSSZiMgyoK169j2qusA95h6gGXjaX9dV1VnALHCaufz1usb4W+WhBp58bydzP9hJTX0zU4Znc9tZ4zllaGZE35Nheq6QJBNVndrRfhG5DrgA+Jp+3qlTCuR6HZbjltFBuTE9zupd1Vz/5EoONjTz9dED+N4ZwxiXmx7qsIzpUNg1c7kjs+4AvqqqdV67FgLPiMhDOB3wI4CVODd1jhCRoThJ5Erg6uBGbYx/vFdUwc1zC+nXO4EXvncqx/bvHeqQjPFJ2CUT4FEgAVjqVudXqOp3VXWTiDyH07HeDNyqqi0AInIbsARnaPBsVd0UmtCN6bqlH+/j1qfXcEzfFObeOJF+vRNDHZIxPgu7ocHBYkODTThZtL6M2+et5YTBfZhz/clBnw7EGF/1mKHBxkSbBetK+a/nPmJ8bjpPXn9yp6O0jAlHlkyMCaGX1pTw4+c/4uT8TGZfdzIpdh+I6aHs/1xjQmTxhjJ+9PxHTD4mi8dnFHw2T5UxPZH932tMCHywrZIfzFvH+Nx0nphxMknxsaEOyZhuiZ7ZyYwJE5vLapg5t5C8rGRmX2eJxEQGSybGBNG28kPMmL2SlIQ45tww0UZtmYhhycSYICncWcWlj72PR5U5N0zsdFVBY3oSSybGBMEbG8u4+vEPyUiO56XvncZxA+zOdhNZrAPemAB7aU0JP3reuY/k8Rknk5liTVsm8lgyMSaA3ti4lx+7w39t1JaJZNbMZUyAvLu1nNufXcu43HT+dm2BJRIT0SyZGBMAhTurmDl3Ncf0TeHv1020O9tNxLNkYoyfrS/Zz/VPrmJgn0T+ceMp9Em2ubZM5LNkYowffbK3hmtnr6RPci+evvkU+vZOCHVIxgSFJRNj/KTo00N86/GVJMbF8sxNkxjYx+4jMdHDkokxfvBR8X6u+OsHgPLUTaeQl5Uc6pCMCSpLJsZ007tby7nqbytISYjlhe+eyvB+qaEOyZigsyEmxnTDu1vLueHvqxjWN5W5N0ykX5ottWuikyUTY7rI41F+/dpmcjKSmT9zso3aMlEtbJu5RORHIqIiku1ui4g8IiJFIrJeRCZ4HTtDRLa6jxmhi9pEk2Wb97Fl30Fu/9pwSyQm6oVlzUREcoFzgN1execCI9zHKcBjwCkikgn8HCgAFFgtIgtVtTq4UZtooqo8+nYReZnJXDh2UKjDMSbkwrVm8nvgDpzk0Go6MFcdK4B0ERkIfB1YqqpVbgJZCkwLesQmqizfWsH6kgPccsYw4mLD9Z+RMcETdv8KRGQ6UKqqHx2xazBQ7LVd4pa1V97Wa88UkUIRKSwvL/dj1CbaPPrPrQzsk8glE3JCHYoxYcHnZi4RSQHqVbWluxcVkWXAgDZ23QP8BKeJy+9UdRYwC6CgoEA7OdyYNn24vZJVO6u598LRxMeF3e8xY0Ki3WQiIjHAlcA1wMlAA5AgIhXAIuCvqlrUlYuq6tR2rjkGGAp8JCIAOcAaEZkIlAK5XofnuGWlwBlHlP+rK3EZ0xmPR7n/jU/ITk3gyol5oQ7HmLDR0c+qt4FhwN3AAFXNVdV+wBRgBXC/iHzLn8Go6gZV7aeq+aqaj9NkNUFV9wILgWvdUV2TgAOqWgYsAc4RkQwRycCp1SzxZ1zGtHppbSlrdu/nzmnHkdjLppQ3plVHzVxTVbXpyEJVrQJeBF4UkWCOh1wMnAcUAXXA9a3xiMivgFXucb90YzTGr2rqm7jv9U8Yn5fOpdZXYswXtJtMVLVJnLamiXzeoV0KrFRVbT0mkMG5tZPW5wrc2s5xs4HZgYzFmEeWbaWytoHZ1xUQEyOhDseYsNJRn8k5wJ+BrThJBJz+iOEicouqvhmE+IwJC1v3HeTv7+/kypNzGZuTHupwjAk7HTVzPYzT1LXTu1BEhuI0OY0KYFzGhJVfvvYxyfGx/Pic40IdijFhqaMO+DicDvAjlQI2d4SJGu/8p5x3t1bw/anHkpVqi10Z05aOaiazgVUiMo/PbwrMA74JPBHowIwJBy0e5X8XbyYvM5lvTxoS6nCMCVsddcD/r4i8gjONyWS3uBS4RlU/DkJsxoTci6tL+GTvQR69erzdoGhMBzq8A15VNwObW7dFZIIlEhMt6hqbeXDpFk7MTef8MQNDHY4xYa2j0VwT2iheKCIXAqKqawIXljGh95d3trOvpoE/XT0Bd0YGY0w7OqqZFOLc6d7gVZYFPIQzm+9ZAYzLmJB6fUMZf/znVi4aN4iC/MxQh2NM2OsomVwO3A48oKqvA4jIDlU9MyiRGRMiK3dU8f356xifm879l44NdTjG9Ajt9iiq6ovA+TjzXj0vInl8cX0RYyLO1n0HuWnOKnIyknhixskkxdv8W8b4orMO+EPAD0VkPDAHSA1KVMYEWYtHeWrFLv7vzS0k9oplzvUTyUiJD3VYxvQYPq1noqprReQsoHeA4zEm6FbvquJnCzaxaU8NU4Zn8+tvnEBuZnKowzKmR+loNNdPgT+3zsDrTrRY47X/LCBZVV8LeJTGBMDqXVU8/FYRy/9TTv+0BB69ejznjxloI7eM6YKOaiYbgFdFpB5YA5QDicAI4ERgGfDbQAdojL+t3lXFQ0v/w3tFlWSlxHPntJFcO3kIKQk+LzxqjDlCR3fALwAWiMgI4DRgIE7N5ClgpqoeDk6IxvjHhpIDPLh0C//aUk52ajz3nDeKayblkRxvScSY7ur0X5GqbsWZht6YHqmx2cPvl/2Hv7yzjT5Jvbhz2khmnDrEkogxfmT/mkxEK/r0IN+ft45Ne2q48uRcfnL+KNISbdJrY/zNkomJSIcbW3jsnW385Z1tpCbEMevbJ3HO8QNCHZYxEcuSiYkoHo+yaEMZ/7t4M3sO1DP9xEHcc/4o+vVODHVoxkS0joYG/5EO7nhX1dsDEpFz7f+Hs957C7BIVe9wy+8GbnTLb1fVJW75NJyVIWOBx1X1vkDFZsJTdW0jzxUW89SHuyiuOsyogWn84crxTBxq82oZEwydTfQIzkiu0cB8d/tyIGDT0IvImThrqIxT1QYR6eeWjwauBI4HBgHLRORY97Q/AWfjrAy5SkQW2lT50aGkuo5Zy7czf1UxDc0eJuZncsfXR3LemIHExtj9IsYES0dDg+cAiMj3gCmq2uxu/wV4N4AxfQ+4T1Ub3Dg+dcunA/Pc8h0iUgRMdPcVqep2N7557rGWTCLY3gP1/G7JFhasK0UELhmfw/VT8hk5IC3UoRkTlXzpM8kA0oAqdzvVLQuUY4GviMhvgHrgx6q6ChiMMyV+qxK3DD5fVri1/JS2XlhEZgIzAfLy8vwctgkGVWXBuj38bMFGGpo9fHvyEG7+yjEMSk8KdWjGRDVfksl9wFoReRsQ4HTg3u5cVESWAW0NrbnHjSkTmAScDDwnIsd053qtVHUWMAugoKDAZkDuYaprG7nnlQ0s3rCXCXnpPHjFiQzNTgl1WMYYOkkmIhIDbMH5pd/6a/9OVd3bnYuq6tQOrvk94CV3LrCVIuIBsnHWn8/1OjTHLaODchMhPthWyQ/nr6OytoE7ph3Hd04fZn0ixoSRzqag94jIn1R1PLAgSDG9ApwJvO12sMcDFcBC4BkReQinA34EsBKntjRCRIbiJJErgauDFKsJsOYWDw+/tZVH3y5iaFYKj884jRMG9wl1WMaYI/jSzPWWiFzK57WFQJsNzBaRjUAjMMO97iYReQ6nY70ZuFVVWwBE5DZgCc7Q4NmquikIcZoAq29q4bZn1rJs8z4uOymHX1x0vE3GaEyYks7yg4gcBFJwvsDrcWoCqqo9ethMQUGBFhYWdn6gCYnDjS3M/Ech726t4JfTj+fayfmhDskYA4jIalUtOLLcl4kebUEsE1Q19U3cPKeQlTureOCysVxRkNv5ScaYkPKpzUBEMnD6KD6bk0JVlwcqKBOdNu05wLMrd/PK2j0cbmrhD988keknDu78RGNMyHWaTETkJuD7OKOk1uEM2f0AOCugkZmoUd/Uwo+e/4hF68tIiIvh/LEDmTE5n3G56aEOzRjjI19qJt/Hud9jhaqeKSIjsRUWjZ9U1TZy05xVrC3ezw+nHst1p+bTJ9mmiDemp/ElmdSrar2IICIJqvqJiBwX8MhMxNtVWct1T65iz/7D/PnqCZw7ZmCoQzLGdJEvyaRERNJx7v9YKiLVwK5ABmUi3+ayGr79xEpaPB6eufkUThpis/sa05P5MprrYvfpve6UKn2ANwIalYloa3ZXc93slSTHxzFv5mSG97MBg8b0dL50wP8KWA68r6rvBD4kE8neK6rg5rmF9OudwD9uPIXczORQh2SM8YMYH47ZDlwFFIrIShF5UESmBzguE4H+vbWCG/6+ityMZJ777mRLJMZEEF+auZ4EnhSRAcAVwI9xpnG3tgnjs/eLKrhp7iqGZqfwzM2TyEyJD3VIxhg/8qWZ63GclRb34SyKdRmwJsBxmQjywbZKbpxTSF5mMk/fdIolEmMikC+jubJwJlDcj7NAVkXrqovGdKTFo8xavp0H39xCfnYKT980iazUhFCHZYwJAJ9Hc4nIKODrOFPDx6pqTqCDMz3Xnv2H+eH8dXy4o4rzxgzgtxePIT3ZaiTGRCpfmrkuAL6Cs8JiOvBPArsGvOnhiqvquPjP73G4sYXfXTaWy07KQcQWsjImkvnSzDUNJ3k8rKp7AhyP6eEO1DVx3ZMraWpRXrn1NEb0t3EaxkSDTocGq+ptwAqcTnhEJElE7BvCfEljs4fvPb2a3VV1/PXbJ1kiMSaKdJpMRORm4AXgr25RDs7UKsZ8RlX56SsbeH9bJfddMpZJx2SFOiRjTBD5ctPircBpQA2Aqm4F+gUyKNOzqCq/em0zzxWWcPvXRnDpSTY2w5ho40syaVDVxtYNEYkDgrEWvOkBVJX739jC7Pd2cP1p+fxw6ohQh2SMCQFfksk7IvITIElEzgaeB14NVEAicqKIrBCRdSJSKCIT3XIRkUdEpEhE1ovIBK9zZojIVvcxI1CxmS9SVX6/bCt/eWcb15ySx88uGG2jtoyJUr4kk7uAcmAD8B1gsareE8CYHgB+oaonAj9ztwHOxVk6eATOdC6PAYhIJvBz4BRgIvBzd5lhE0CNzR5+8vIGHnlrK1cU5PCr6SdYIjEmivkymsujqn9T1ctV9TJgl4gsDWBMCqS5z/sArcORpwNz1bECSBeRgTg3Ui5V1SpVrQaW4gxnNgGyv66Ra2d/yLMri7n1zGHcd8lYYmIskRgTzdq9z0REzgL+AgzCGb11P/AkIMBvAhjTD4AlIvJ/OMnuVLd8MFDsdVyJW9Ze+ZeIyEycWg15eXl+DTpaVNU2culj71NafZiHrhjHJROss90Y0/FNiw/ifPF+gNPE9AFwl6o+2t2LisgyYEAbu+4Bvgb8UFVfFJErgCeAqd29JoCqzgJmARQUFNgggi742YKNlFTX8fRNk5g41FZHNMY4Okomqqr/cp+/IiKl/kgk7gu3mxxEZC7wfXfzeeBx93kpkOt1aI5bVgqccUT5v/wRp/miRevLeG19Gf/99eMskRhjvqCjZJIuIpd4H+u9raovBSimPcBXcRLCWcBWt3whcJuIzMPpbD+gqmUisgT4rVen+znA3QGKLWpVHGrgfxZsZGxOH75z+jGhDscYE2Y6SibvABd6bS/32lYgUMnkZuBh936Wetw+DmAxcB5QBNQB1wOoapW7tPAq97hfqmpVgGKLSqrK/7yykUP1zTx4+TjiYn0ZBGiMiSbtJhNVvT6YgXhd99/ASW2UK87d+G2dMxuYHeDQotbbWz7l9Y17uXPaSJtvyxjTJvuJaTrk8Si/W/IfhmQlc9NXhoY6HGNMmLJkYjq0aEMZm8tq+K+zj6WXNW8ZY9ph3w6mXc0tHh5a+h9GDujNhWMHhTocY0wY82UK+ltFJN1rO0NEbgloVCYsvLimhB0VtfzonOPsDndjTId8qZncrKr7WzfcKUtuDlhEJizUN7Xw8LKtnJibztRRtuKAMaZjviSTWPGawU9EYoH4wIVkwsFf3tnGngP13PH142wCR2NMp3xZA/4NYL6ItK60+B23zESoLXsP8qe3i7ho3CBOHZ4d6nCMMT2AL8nkTpwE8j13eymfT3FiIkyLR7njxfX0TuzFzy8cHepwjDE9RKfJRFU9OGuHPBb4cEyozf73Dj4q3s8jV40nKzUh1OEYY3qIjqagf05VrxCRDbSxTK+qjg1oZCaoKg818NbmT3lw6RamjurPhWMHhjokY0wP0lHNpHXm3guCEYgJjS17D/LTVzZQuKsaVRiancKvv2GrJhpjjk5Hc3OVuX93BS8cE0yqyl0vrWdnRS23nzWCs0f35/hBaZZIjDFHraNmroO00bzVSlXT2ttneobFG/aydvd+Hrh0LFecnNv5CcYY046Oaia9Adzp3cuAf+As2XsNYA3qPVxjs4cHlnzCyAG9ufQkW3rXGNM9vty0eJGq/llVD6pqjao+BkwPdGAmsJ7+cBe7Kuu469yRxNpUKcaYbvIlmdSKyDUiEisiMSJyDVAb6MBM4Bw43MQjb21lyvBsvnps31CHY4yJAL4kk6uBK4B9wKfA5W6Z6aGe+PcOquuauOvckdbZbozxC19uWtyJNWtFjIbmFp5esYupo/pxwuA+oQ7HGBMhfJmCPkdEXhaRT93HiyJiPbY91KL1ZVTWNjLj1PxQh2KMiSC+NHM9CSwEBrmPV92yLhORy0Vkk4h4RKTgiH13i0iRiGwRka97lU9zy4pE5C6v8qEi8qFbPl9EbEbjDsx5fyfD+qYwxSZwNMb4kS/JpK+qPqmqze7j70B3e203ApcAy70LRWQ0cCVwPDAN+LPb8R8L/Ak4FxgNXOUeC3A/8HtVHQ5UAzd2M7aItXZ3NR+VHGDGqfnWV2KM8StfkkmliHyr9UtdRL4FVHbnoqq6WVW3tLFrOjBPVRtUdQdQBEx0H0Wqul1VG4F5wHR3nZWzgBfc8+cA3+hObJFszvs7SU2I45IJ1kppjPEvX5LJDTijufbi3Lx4GXB9gOIZDBR7bZe4Ze2VZwH7VbX5iPI2ichMESkUkcLy8nK/Bh7uPj1Yz6INZVx2Ug6pCb6sPGCMMb7zZTTXLuCio31hEVkGDGhj1z2quuBoX88fVHUWMAugoKCg3aliItGzHxbT1KJcO3lIqEMxxkSgjubmukNVHxCRP9L2FPS3d/TCqjq1C/GUAt6TROW4ZbRTXgmki0icWzvxPt64GppbeOrDXXz12L4c0zc11OEYYyJQRzWTze7fwmAE4loIPCMiD+GMHBsBrMSZE2yEiAzFSRZXAlerqorI2zhNb/OAGUBIaj3hbNH6MsoPNnDj5UNDHYoxJkJ1NNHjq+7fOa1lIhIDpKpqTXcuKiIXA3/EGRW2SETWqerXVXWTiDwHfAw0A7eqaot7zm3AEiAWmK2qm9yXuxOYJyK/BtYCT3Qntkijqjzx7x0M75fKV0bYcGBjTGB02mciIs8A3wVagFVAmog8rKq/6+pFVfVl4OV29v0G+E0b5YuBxW2Ub8cZ7WXasGpnNZv21PCbi23BK2NM4Pgymmu0WxP5BvA6MBT4diCDMv7z5Hs76JPUi0vG23BgY0zg+JJMeolIL5xkslBVm+hg0SwTPoqr6liyaS9Xn5JHUnxsqMMxxkQwX5LJX4GdQAqwXESGAN3qMzHB8Y8VuxARGw5sjAk4X+4zeQR4xKtol4icGbiQjD94PMrCdXs4a2Q/BvZJCnU4xpgI58uswVki8oiIrBGR1SLyMGBzl4e5tcXV7K2p5/wxtsKyMSbwfGnmmgeUA5fi3M9RDswPZFCm+xZv2Et8bAxnjeoX6lCMMVHAl0maBqrqr7y2fy0i3wxUQKb7VJXXN5Rx+rHZpCX2CnU4xpgo4EvN5E0RudJd/z1GRK7AuXnQhKmPSg6w50A9555gTVzGmODwJZncDDwDNLiPecB3ROSgiNiorjC0eEMZvWKFqaP7hzoUY0yU8GU0V+9gBGL8Q1VZvKGMKcOz6ZNkTVzGmOBot2biLoLV+vy0I/bdFsigTNdtLK2hpPow59ooLmNMEHXUzPVfXs//eMS+GwIQi/GDRRvKiIsRzrEmLmNMEHWUTKSd521tmzDQ4lFe/WgPpw3PJj05PtThGGOiSEfJRNt53ta2CQPvFVVQuv8wlxfYpI7GmODqqAN+pIisx6mFDHOf424fE/DIzFGbX1hMRnIvzrYmLmNMkHWUTEYFLQrTbVW1jSzdtI9rJuWREGczBBtjgqujlRZ3BTMQ0z0vry2lscXDN0/ODXUoxpgo5MtNiybMqSrPrSpmXE4fRg5IC3U4xpgoZMkkAnxUcoAt+w5yhdVKjDEhEpJkIiKXi8gmEfGISIFX+dnuNPcb3L9nee07yS0vcqfEF7c8U0SWishW929GKN5TKM1fVUxirxguHDco1KEYY6JUl5KJiNzbzetuBC4Blh9RXgFcqKpjgBnAP7z2PYYzT9gI9zHNLb8LeEtVRwBvudtR41BDMwvXlXLemIE2Q7AxJmS6WjNZ3Z2LqupmVd3SRvlaVd3jbm4CkkQkQUQGAmmqukJVFZiLsyY9wHRgjvt8jld5VHhlbSm1jS18a5ItzWuMCZ0uJRNVfdXfgbThUmCNqjYAg4ESr30lbhlAf1Utc5/vBdq9yUJEZopIoYgUlpeXByLmoFJVnlqxi9ED0xifmx7qcIwxUazTWYNF5JE2ig8Ahaq6oIPzlgED2th1T0fnueceD9wPnNNZfN5UVUWk3bvzVXUWMAugoKCgx9/Fv2Z3NZ/sPchvLx6D24VkjDEh4ctKi4nASOB5d/tSYAcwTkTOVNUftHWSqk7tSkAikgO8DFyrqtvc4lLAe46QHLcMYJ+IDFTVMrc57NOuXLcnemrFblIT4ph+onW8G2NCy5dmrrHAmar6R1X9IzAVJ7lczFHWHDojIunAIuAuVX2vtdxtxqoRkUnuKK5rgdbazUKcznrcvx3WeiJFVW0ji9aXccmEwaQk+PKbwBhjAseXZJIBpHptpwCZqtqCs/LiURORi0WkBJgMLBKR1mWAbwOGAz8TkXXuo5+77xbgcaAI2Aa87pbfB5wtIltxEt19XYnJV/VNLXxaUx/IS/jk+cJiGls81vFujAkLvvykfQBYJyL/wpnk8XTgtyKSAizrykVV9WWcpqwjy38N/LqdcwqBE9oorwS+1pU4uuLGOauoa2zh5VtO6/zgAPF4lGdW7mZifibH9reFMI0xoddpzURVnwBOBV7BSQBTVPVxVa1V1f8OcHxhJyc9meKqwyGNYfnWcnZV1vGtyVYrMcaEB19Gc70KPAMsVNXawIcU3nIzk6g41EB9UwuJvUIzO+9TK3aRnRrPtOPbGixnjDHB50ufyf8BXwE+FpEXROQyEUkMcFxhKycjGYCS6tDUToqr6njrk0+58uQ84uNsajVjTHjwpZnrHVW9BWdBrL8CVxBFw2+PlJORBEBJdV1Irv/syt0IcNUpeSG5vjHGtMWnMaUikgRcCHwTmMDn05dEnVDWTBqaW5i/qpipo/ozOD0p6Nc3xpj2+NJn8hwwEXgDeBR4R1U9gQ4sXPXrnUB8bExIkskbG/dSWdvIt63j3RgTZnypmTwBXOXeV4KITBGRq1T11sCGFp5iYoTBGUkUB7mZS1WZ8/5OhmancNqw7KBe2xhjOuNLn8kSYKyIPCAiO4FfAZ8EOrBwlpORFPSaycKP9rBm935umDKUmBibh8sYE17arZmIyLHAVe6jApgPiKqeGaTYwlZORhJLP94XtOtVHmrgF69+zIm56Vw90TrejTHhp6Nmrk+Ad4ELVLUIQER+GJSowlxORjIVhxo53NhCUnzg7zX55Wsfc7C+iQcuG0us1UqMMWGoo2auS4Ay4G0R+ZuIfA1nOpWo1zo8uHR/4PtN/vnJPhas28MtZwy3qVOMMWGr3WSiqq+o6pU4MwS/DfwA6Ccij4mIX2cL7mlahwcHelqVusZmfvryRo7tn8otZw4L6LWMMaY7fOmAr1XVZ1T1Qpx1RNYCdwY8sjCWG6QbF//yr23sOVDPby8eQ0JcaKZuMcYYXxzVfByqWq2qs1Q1aLP0hqPs1ATi4wJ7r0lJdR1/Xb6di8YNoiA/M2DXMcYYf7DJnbogJkbISQ/s8OD739iCCNx57siAXcMYY/zFkkkXDc5IClgzV+HOKl79aA8zTx9m06YYY3oESyZdlJuZHJCaicej/PK1jxmQlsh3v3qM31/fGGMCwZJJF+VkJFFZ20htQ7NfX3fuBztZX3KAu84dSXK8re1ujOkZLJl0Uevw4NL9/qudFFfVcf8bWzjjuL5MP3GQ317XGGMCLSTJREQuF5FNIuIRkYI29ueJyCER+bFX2TQR2SIiRSJyl1f5UBH50C2fLyLxwXgP/l7XRFW566X1xMYIv714DCJ2f6gxpucIVc1kI84d9svb2f8Q8HrrhojEAn8CzgVGA1eJyGh39/3A71V1OFAN3BiooL19nkz8UzOZv6qY94oqufu8kQyyTndjTA8TkmSiqptVdUtb+0TkG8AOYJNX8USgSFW3q2ojMA+YLs7P97OAF9zj5gDfCFTc3vqmJpDgp3tN9tXU85tFm5l8TBZXnWwTORpjep6w6jMRkVScu+t/ccSuwUCx13aJW5YF7FfV5iPK23v9mSJSKCKF5eXl3Y2VnIwkiqu638x13+uf0NDs4X8vGWPTyxtjeqSAJRMRWSYiG9t4TO/gtHtxmqwOBSIm9+79AlUt6Nu3b7dfLycjuduLZK3eVcXLa0u5+fSh5GendDsmY4wJhYCNPVXVqV047RTgMhF5AEgHPCJSD6wGcr2OywFKgUogXUTi3NpJa3lQjBqYxuPvbmd/XSPpyUff79/iUe5d6NxTcssZwwMQoTHGBEdYNXOp6ldUNV9V84E/AL9V1UeBVcAId+RWPHAlsFBVFWdG48vcl5gBLAhWvBeMHUizR3l9494unf98YTEbSg9w93kjSUmwe0qMMT1XqIYGXywiJcBkYJGILOnoeLfWcRuwBNgMPKeqrR30dwL/JSJFOH0oTwQu8i86flAax2SnsHDdnqM+t/JQA79bsoWT8zO4aJzdU2KM6dlC8nNYVV8GXu7kmHuP2F4MLG7juO04o72CTkS4YNwg/vjPrXxaU0+/tESfzis7cJhvP7GSgw3N3HvR8XZPiTGmxwurZq6e6KJxg1CF19aX+XT8jopaLnvsA/YeqGfuDRM5flCfAEdojDGBZw313TS8XyqjB6bx6vo93DBlaJvHNLV4WF9ygPeLKpjzwU48CvNmTuKEwZZIjDGRwZKJH1w4bhD3v/EJxVV15GYmf2HfB9squXluIYfcCSHH5fThwStOZHi/1FCEaowxAWHNXH5wwdiBACz86Msd8Us27aXFo/z5mgms+Z+zWXDbFEskxpiIY8nED3Izk5mQl86rbSSTtburGZvTh/PGDCQzJShzUBpjTNBZMvGTaScM4JO9B9lXU/9ZWX1TC5v21DA+LyOEkRljTOBZMvGTU4dlA04fSatNew7Q7FHG56WHKCpjjAkOSyZ+MmpgGn2SevH+torPytbu3g/A+Nz00ARljDFBYsnET2JjhEnHZPK+V81k7e79DE5P8vlmRmOM6aksmfjRqcOyKak+/Nm09OuK91sTlzEmKlgy8aNTh2UB8P62CvbV1FO6/7B1vhtjooLdtOhHw/ulkp2awPvbKumT5AwDtpqJMSYaWDLxIxHh1GFZvL+tkgFpicTHxnD8oLRQh2WMMQFnzVx+duqwLMoPNrBg3R5GD0ojIS421CEZY0zAWTLxs9b7TfbW1FsTlzEmalgy8bPczCQGpycBWOe7MSZqWDLxs9Z+E7CbFY0x0cM64APgxq8MJTczmZyMpFCHYowxQWHJJABGDkhj5AAbxWWMiR4haeYSkctFZJOIeESk4Ih9Y0XkA3f/BhFJdMtPcreLROQRcRdOF5FMEVkqIlvdv9ZRYYwxQRaqPpONwCXAcu9CEYkDngK+q6rHA2cATe7ux4CbgRHuY5pbfhfwlqqOAN5yt40xxgRRSJKJqm5W1S1t7DoHWK+qH7nHVapqi4gMBNJUdYWqKjAX+IZ7znRgjvt8jle5McaYIAm30VzHAioiS0RkjYjc4ZYPBkq8jitxywD6q2qZ+3wv0L+9FxeRmSJSKCKF5eXl/o7dGGOiVsA64EVkGTCgjV33qOqCDuKZApwM1AFvichq4IAv11RVFRHtYP8sYBZAQUFBu8cZY4w5OgFLJqo6tQunlQDLVbUCQEQWAxNw+lFyvI7LAUrd5/tEZKCqlrnNYZ92I2xjjDFdEG7NXEuAMSKS7HbGfxX42G3GqhGRSe4ormuB1trNQmCG+3yGV7kxxpggCdXQ4ItFpASYDCwSkSUAqloNPASsAtYBa1R1kXvaLcDjQBGwDXjdLb8POFtEtgJT3W1jjDFBJM7gqOgjIuXAri6eng1UdHpU5InG9x2N7xmi833be/bNEFXte2Rh1CaT7hCRQlUt6PzIyBKN7zsa3zNE5/u299w94dZnYowxpgeyZGKMMabbLJl0zaxQBxAi0fi+o/E9Q3S+b3vP3WB9JsYYY7rNaibGGGO6zZKJMcaYbrNkcpREZJqIbHHXVYnI6e5FJFdE3haRj911Zb7vlkf82jEiEisia0XkNXd7qIh86H7e80UkPtQx+puIpIvICyLyiYhsFpHJkf5Zi8gP3f+3N4rIsyKSGImftYjMFpFPRWSjV1mbn604HnHf/3oRmXA017JkchREJBb4E3AuMBq4SkRGhzaqgGgGfqSqo4FJwK3u+4yGtWO+D2z22r4f+L2qDgeqgRtDElVgPQy8oaojgXE47z9iP2sRGQzcDhSo6glALHAlkflZ/53P135q1d5ney6frxc1E2cNKZ9ZMjk6E4EiVd2uqo3APJz1VCKKqpap6hr3+UGcL5fBRPjaMSKSA5yPM20P7jxwZwEvuIdE4nvuA5wOPAGgqo2qup8I/6xxJrlNcucATAbKiMDPWlWXA1VHFLf32U4H5qpjBZDuTp7rE0smR2cwUOy17b2uSkQSkXxgPPAhR7F2TA/1B+AOwONuZwH7VbXZ3Y7Ez3soUA486TbvPS4iKUTwZ62qpcD/AbtxksgBYDWR/1m3au+z7db3myUT0y4RSQVeBH6gqjXe+9wVLyNmXLmIXAB8qqqrQx1LkMXhLPPwmKqOB2o5okkrAj/rDJxf4UOBQUAKX24Kigr+/GwtmRydUiDXa9t7XZWIIiK9cBLJ06r6klu8r7XaG4Frx5wGXCQiO3GaL8/C6UtId5tCIDI/7xKgRFU/dLdfwEkukfxZTwV2qGq5qjYBL+F8/pH+Wbdq77Pt1vebJZOjswoY4Y76iMfptFsY4pj8zu0reALYrKoPee2K2LVjVPVuVc1R1Xycz/WfqnoN8DZwmXtYRL1nAFXdCxSLyHFu0deAj4ngzxqneWuSu26S8Pl7jujP2kt7n+1C4Fp3VNck4IBXc1in7A74oyQi5+G0rccCs1X1N6GNyP9EZArwLrCBz/sPfoLTb/IckIczff8Vqnpk516PJyJnAD9W1QtE5BicmkomsBb4lqo2hDA8vxORE3EGHcQD24HrcX5oRuxnLSK/AL6JM3JxLXATTv9ARH3WIvIscAbOVPP7gJ8Dr9DGZ+sm1kdxmvzqgOtVtdDna1kyMcYY013WzGWMMabbLJkYY4zpNksmxhhjus2SiTHGmG6zZGKMMabbLJkY4yci0iIi67weHU6OKCLfFZFr/XDdnSKS3d3XMaY7bGiwMX4iIodUNTUE192JMwNuRbCvbUwrq5kYE2BuzeEBEdkgIitFZLhbfq+I/Nh9fru7fsx6EZnnlmWKyCtu2QoRGeuWZ4nIm+56HI8D4nWtb7nXWCcif3WXTTAm4CyZGOM/SUc0c33Ta98BVR2Dc4fxH9o49y5gvKqOBb7rlv0CWOuW/QSY65b/HPi3qh4PvIxzJzMiMgrnru7TVPVEoAW4xp9v0Jj2xHV+iDHGR4fdL/G2POv19/dt7F8PPC0ir+BMdwEwBbgUQFX/6dZI0nDWH7nELV8kItXu8V8DTgJWOTNjkERkTdBowpglE2OCQ9t53up8nCRxIXCPiIzpwjUEmKOqd3fhXGO6xZq5jAmOb3r9/cB7h4jEALmq+jZwJ9AHSMWZbPMa95gzgAp3XZnlwNVu+blA6/rsbwGXiUg/d1+miAwJ3Fsy5nNWMzHGf5JEZJ3X9huq2jo8OENE1gMNwFVHnBcLPOUuoSvAI6q6X0TuBWa759Xx+bThvwCeFZFNwPs4U6qjqh+LyE+BN90E1QTcijMzrDEBZUODjQkwG7prooE1cxljjOk2q5kYY4zpNquZGGOM6TZLJsYYY7rNkokxxphus2RijDGm2yyZGGOM6bb/D7yEz6nGSFm7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actor_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the weights\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mactor_model\u001b[49m\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights/pendulum_actor.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m critic_model\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights/pendulum_critic.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m target_actor\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights/pendulum_target_actor.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'actor_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the weights\n",
    "actor_model.save_weights(\"Weights/pendulum_actor.h5\")\n",
    "critic_model.save_weights(\"Weights/pendulum_critic.h5\")\n",
    "\n",
    "target_actor.save_weights(\"Weights/pendulum_target_actor.h5\")\n",
    "target_critic.save_weights(\"Weights/pendulum_target_critic.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
